{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Specify the folder containing your Excel files\n",
    "data_folder = \"../../data\"\n",
    "\n",
    "# Define the column names\n",
    "col_names = ['Case ID', 'Suspect Product Active Ingredients', 'Reason for Use', 'Reactions', 'Serious', 'Outcomes', 'Sex', 'Patient Age', 'Patient Weight']\n",
    "\n",
    "# Initialize an empty DataFrame to store the combined data\n",
    "combined_data = pd.DataFrame(columns=col_names)\n",
    "\n",
    "# Iterate over all Excel files in the folder\n",
    "for file in os.listdir(data_folder):\n",
    "    if file.endswith(\".xlsx\"):  # Check if the file is an Excel file\n",
    "        file_path = os.path.join(data_folder, file)\n",
    "        data = pd.read_excel(file_path, usecols=col_names)  # Load only the specified columns\n",
    "        combined_data = pd.concat([combined_data, data], ignore_index=True)\n",
    "\n",
    "# Display the combined dataset\n",
    "print(combined_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = ['Case ID', 'Suspect Product Active Ingredients', 'Reactions',  'Serious', 'Sex', 'Patient Age', 'Patient Weight']\n",
    "\n",
    "#df = data[selected_columns]\n",
    "df = combined_data[selected_columns]\n",
    "\n",
    "df.head()\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Suspect Product Active Ingredients'] = df['Suspect Product Active Ingredients'].str.split(';')\n",
    "df_split_temp = df.explode('Suspect Product Active Ingredients', ignore_index=True)\n",
    "\n",
    "df_split_temp['Reactions'] = df_split_temp['Reactions'].str.split(';')\n",
    "df_split = df_split_temp.explode('Reactions', ignore_index = True)\n",
    "\n",
    "df_split.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = df_split['Serious'].value_counts()\n",
    "\n",
    "print(count)\n",
    "\n",
    "print(df_split.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_multi = pd.get_dummies(df_split, columns = ['Suspect Product Active Ingredients', 'Reactions'], prefix=['Product', 'Reaction'], prefix_sep='_')\n",
    "reactions = [col for col in df_multi.columns if col.startswith(\"Reaction_\")]\n",
    "df_reaction = df_multi[reactions]\n",
    "df_multi = df_multi.groupby('Case ID').max().reset_index()\n",
    "\n",
    "columns_to_exclude = ['Case ID', 'Suspect Product Active Ingredients', 'Reactions', 'Serious', 'Sex', 'Patient Age', 'Patient Weight']\n",
    "\n",
    "columns_to_convert = [col for col in df_multi.columns if col.startswith(\"Product_\") or col.startswith(\"Reaction_\")] #not in columns_to_exclude]\n",
    "\n",
    "df_multi[columns_to_convert] = df_multi[columns_to_convert].astype(int)\n",
    "\n",
    "df_final = df_multi\n",
    "print(df_multi.columns)\n",
    "print(df_final)\n",
    "\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_reaction.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded = df_multi.copy()\n",
    "print(df_encoded.columns)\n",
    "#print(df_split['Patient Age'])\n",
    "df_encoded['Patient Age'] = df_encoded['Patient Age'].astype(str)\n",
    "df_encoded['Patient Age'] = df_encoded['Patient Age'].str.replace(r'\\D+', '', regex=True)\n",
    "df_encoded['Patient Age'] = pd.to_numeric(df_encoded['Patient Age'], errors='coerce')  # Converts to numeric, sets invalid values to NaN\n",
    "\n",
    "df_encoded['Patient Weight'] = df_encoded['Patient Weight'].replace('Not Specified', \"0 KG\")\n",
    "df_encoded['Patient Weight'] = df_encoded['Patient Weight'].astype(str)\n",
    "df_encoded['Patient Weight'] = df_encoded['Patient Weight'].str.replace(r'[^\\d.]', '', regex=True)\n",
    "df_encoded['Patient Weight'] = pd.to_numeric(df_encoded['Patient Weight'], errors='coerce')\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "#df_encoded['Suspect Product Active Ingredients'] = label_encoder.fit_transform(df_encoded['Suspect Product Active Ingredients'])\n",
    "df_encoded['Sex'] = label_encoder.fit_transform(df_encoded['Sex'])\n",
    "df_encoded['Serious'] = label_encoder.fit_transform(df_encoded['Serious'])\n",
    "print(df_encoded.isnull().sum())\n",
    "df_encoded.dropna(inplace=True)\n",
    "print(df_encoded.isnull().sum())\n",
    "df_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = df_encoded['Serious'].value_counts()\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum columns that start with \"Reaction_\"\n",
    "reaction_columns = df_encoded.filter(like=\"Reaction_\", axis=1)\n",
    "column_sums = reaction_columns.sum(axis=0)\n",
    "\n",
    "print(len(reaction_columns.columns))\n",
    "# Set the threshold\n",
    "threshold = 50 #300\n",
    "\n",
    "# Filter column names with sums greater than the threshold\n",
    "columns_above_threshold = column_sums[column_sums > threshold].index.tolist()\n",
    "\n",
    "print(columns_above_threshold)\n",
    "print(len(columns_above_threshold))\n",
    "\n",
    "filtered_df = df_encoded[df_encoded[columns_above_threshold].sum(axis=1) > 0]\n",
    "\n",
    "# Print the number of rows before and after filtering\n",
    "print(f\"Number of rows before filtering: {df_encoded.shape[0]}\")\n",
    "print(f\"Number of rows after removing all-zero rows: {filtered_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign Features/Predictions for Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_exclude = ['Case ID', 'Suspect Product Active Ingredients', 'Serious' ,'Reactions']\n",
    "\n",
    "feature_cols = [col for col in df_encoded.columns if col not in cols_to_exclude and not col.startswith(\"Reaction_\")]\n",
    "\n",
    "print(feature_cols)\n",
    "print(len(feature_cols))\n",
    "\n",
    "X_serious = df_encoded[feature_cols]\n",
    "Y_serious = df_encoded.Serious\n",
    "\n",
    "# Reaction Predictor\n",
    "\n",
    "feat_cols = [\"Serious\"]\n",
    "feat_cols.extend(feature_cols)\n",
    "\n",
    "print(feat_cols)\n",
    "print(len(feat_cols))\n",
    "\n",
    "predict_cols = []#[\"Serious\"]\n",
    "reaction_cols = columns_above_threshold #[col for col in df_encoded.columns if col.startswith(\"Reaction_\")]\n",
    "predict_cols.extend(reaction_cols)\n",
    "\n",
    "print(predict_cols)\n",
    "print(len(predict_cols))\n",
    "\n",
    "df_filtered = df_encoded[df_encoded[predict_cols].sum(axis=1) > 0]\n",
    "\n",
    "X_reaction = df_filtered[feat_cols]\n",
    "Y_reaction = df_filtered[predict_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_serious.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_serious.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reaction.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_reaction.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split Serious Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Address class imbalance using SMOTE\n",
    "smote = SMOTE()\n",
    "X_resampled_serious, y_resampled_serious = smote.fit_resample(X_serious, Y_serious)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_resampled_serious = scaler.fit_transform(X_resampled_serious)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_serious, X_test_serious, y_train_serious, y_test_serious = train_test_split(X_resampled_serious, y_resampled_serious, test_size=0.4, random_state=26)\n",
    "\n",
    "X_train_serious, X_val_serious, y_train_serious, y_val_serious = train_test_split(X_train_serious, y_train_serious, test_size=0.2, random_state=26)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "!pip install torch -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Convert data to torch tensors\n",
    "class Data(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        #self.X = torch.from_numpy(X.to_numpy().astype(np.float32))\n",
    "        #self.y = torch.from_numpy(y.to_numpy().astype(np.float32))\n",
    "        self.X = torch.from_numpy(X.astype(np.float32))\n",
    "        self.y = torch.from_numpy(y.to_numpy().astype(np.float32))\n",
    "        self.len = self.X.shape[0]\n",
    "       \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n",
    "   \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "   \n",
    "batch_size = 32\n",
    "\n",
    "# Instantiate training and test data\n",
    "train_data = Data(X_train_serious, y_train_serious)\n",
    "#train_data = Data(X_resampled, y_resampled)\n",
    "train_dataloader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_data = Data(X_test_serious, y_test_serious)\n",
    "test_dataloader = DataLoader(dataset=test_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "valid_data = Data(X_val_serious, y_val_serious)\n",
    "valid_dataloader = DataLoader(dataset=valid_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Check it's working\n",
    "for batch, (X, y) in enumerate(train_dataloader):\n",
    "    print(f\"Batch: {batch+1}\")\n",
    "    print(f\"X shape: {X.shape}\")\n",
    "    print(f\"y shape: {y.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "input_dim = len(feature_cols) #393\n",
    "hidden_dim = 1024 #512 #400\n",
    "output_dim = 1\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        \n",
    "        # Define multiple layers\n",
    "        self.layer_1 = nn.Linear(input_dim, hidden_dim // 16)\n",
    "        nn.init.kaiming_uniform_(self.layer_1.weight, nonlinearity=\"relu\")\n",
    "        \n",
    "        self.layer_2 = nn.Linear(hidden_dim // 16, hidden_dim // 32)  # Hidden layer (reduce size)\n",
    "        nn.init.kaiming_uniform_(self.layer_2.weight, nonlinearity=\"relu\")\n",
    "        \n",
    "        self.layer_3 = nn.Linear(hidden_dim // 32, hidden_dim // 64)  # Another hidden layer\n",
    "        nn.init.kaiming_uniform_(self.layer_3.weight, nonlinearity=\"relu\")\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        \n",
    "        self.output_layer = nn.Linear(hidden_dim // 64, output_dim)  # Final output layer\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pass through layers with ReLU activations\n",
    "        x = torch.nn.functional.relu(self.layer_1(x))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = torch.nn.functional.relu(self.layer_2(x))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = torch.nn.functional.relu(self.layer_3(x))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = torch.nn.functional.sigmoid(self.output_layer(x))  # Sigmoid for binary output\n",
    "        return x\n",
    "       \n",
    "serious_model = NeuralNetwork(input_dim, hidden_dim, output_dim)\n",
    "print(serious_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(serious_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 15\n",
    "train_loss_values = []\n",
    "valid_loss_values = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    serious_model.train()\n",
    "    train_loss = 0.0\n",
    "    for X, y in train_dataloader:\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "       \n",
    "        # forward pass\n",
    "        pred = serious_model(X)\n",
    "        loss = loss_fn(pred, y.unsqueeze(-1))\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # average training loss for the epoch\n",
    "    train_loss /= len(train_dataloader)\n",
    "    train_loss_values.append(train_loss)\n",
    "\n",
    "    # Validation phase\n",
    "    serious_model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_val, y_val in valid_dataloader:\n",
    "            pred = serious_model(X_val)\n",
    "            loss = loss_fn(pred, y_val.unsqueeze(-1))\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    # Average validation loss for the epoch\n",
    "    val_loss /= len(valid_dataloader)\n",
    "    valid_loss_values.append(val_loss)\n",
    "\n",
    "    # Print loss for the epoch\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(1, num_epochs + 1), train_loss_values, label='Train Loss')\n",
    "plt.plot(range(1, num_epochs + 1), valid_loss_values, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools  # Import this at the top of your script\n",
    "\n",
    "# Initialize required variables\n",
    "y_pred = []\n",
    "y_test = []\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "\"\"\"\n",
    "We're not training so we don't need to calculate the gradients for our outputs\n",
    "\"\"\"\n",
    "with torch.no_grad():\n",
    "    for X, y in test_dataloader:\n",
    "        outputs = serious_model(X)  # Get model outputs\n",
    "        predicted = np.where(outputs.numpy() < 0.45, 0, 1)  # Convert to NumPy and apply threshold\n",
    "        predicted = list(itertools.chain(*predicted))  # Flatten predictions\n",
    "        y_pred.append(predicted)  # Append predictions\n",
    "        y_test.append(y.numpy())  # Append true labels as NumPy\n",
    "        total += y.size(0)  # Increment total count\n",
    "        correct += (predicted == y.numpy()).sum().item()  # Count correct predictions\n",
    "\n",
    "print(f'Accuracy of the network on the test instances: {100 * correct // total}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "y_pred = list(itertools.chain(*y_pred))\n",
    "y_test = list(itertools.chain(*y_test))\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_matrix = confusion_matrix(y_test, y_pred)\n",
    "cf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate ROC curve metrics\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "\n",
    "# Calculate AUC (Area Under the Curve)\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "# Plot the ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color=\"blue\", label=f\"ROC Curve (AUC = {roc_auc:.2f})\")\n",
    "plt.plot([0, 1], [0, 1], color=\"gray\", linestyle=\"--\")  # Diagonal line (random performance)\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Receiver Operating Characteristic (ROC) Curve\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split Reaction Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "\n",
    "def ml_smote(X, Y, k=5, target_samples=None):\n",
    "    \"\"\"\n",
    "    MLSMOTE implementation for multi-label datasets.\n",
    "    \n",
    "    Parameters:\n",
    "        X (array): Feature matrix (NumPy array or DataFrame).\n",
    "        Y (array): Multi-label matrix (NumPy array or DataFrame).\n",
    "        k (int): Number of nearest neighbors.\n",
    "        n_samples (int): Number of synthetic samples to generate.\n",
    "    \n",
    "    Returns:\n",
    "        X_augmented, Y_augmented: Augmented feature and label matrices.\n",
    "    \"\"\"\n",
    "    # Calculate imbalance ratio per label (IRPL)\n",
    "    label_frequencies = Y.sum(axis=0)\n",
    "    irpl = max(label_frequencies) / label_frequencies\n",
    "    \n",
    "    # Calculate mean imbalance ratio (MIR)\n",
    "    mir = np.mean(irpl)\n",
    "    \n",
    "    # Identify tail labels (labels with IRPL > MIR)\n",
    "    tail_labels = np.where(irpl > mir)[0]\n",
    "    print(tail_labels)\n",
    "    \n",
    "    # Identify minority samples\n",
    "    #minority_indices = np.where(Y.sum(axis=1) < Y.shape[1] / 2)[0]  # Example threshold\n",
    "    minority_indices = np.where((Y[:, tail_labels].sum(axis=1)) > 0)[0]\n",
    "    X_minority = X[minority_indices]\n",
    "    Y_minority = Y[minority_indices]\n",
    "    \n",
    "    # Fit nearest neighbors\n",
    "    knn = NearestNeighbors(n_neighbors=k).fit(X_minority)\n",
    "    synthetic_X, synthetic_Y = [], []\n",
    "\n",
    "    # Calculate target samples for each tail label\n",
    "    if target_samples is None:\n",
    "        target_samples = (label_frequencies.max() - label_frequencies).astype(int)\n",
    "\n",
    "    #for _ in range(n_samples):\n",
    "    for _ in range(target_samples.sum()):\n",
    "        # Randomly select a minority sample\n",
    "        idx = np.random.choice(len(X_minority))\n",
    "        x = X_minority[idx]\n",
    "        labels = Y_minority[idx]\n",
    "\n",
    "        # Find k-nearest neighbors\n",
    "        neighbors = knn.kneighbors([x], return_distance=False)[0]\n",
    "        #neighbor_idx = np.random.choice(neighbors[1:])  # Avoid selecting the sample itself\n",
    "        selected_neighbors = neighbors[1:]  # Exclude itself\n",
    "\n",
    "        # Interpolate between the sample and a neighbor\n",
    "        #neighbor_x = X_minority[neighbor_idx]\n",
    "        #synthetic_x = x + np.random.rand() * (neighbor_x - x)\n",
    "        #synthetic_y = np.logical_or(labels, Y_minority[neighbor_idx]).astype(int)\n",
    "\n",
    "        # Generate synthetic features by averaging neighbors\n",
    "        weights = np.random.dirichlet(np.ones(len(selected_neighbors)))\n",
    "        neighbor_x = np.dot(weights, X_minority[selected_neighbors])\n",
    "        synthetic_x = x + np.random.rand() * (neighbor_x - x)\n",
    "\n",
    "        # Generate synthetic labels probabilistically\n",
    "        synthetic_y = (np.random.rand(len(labels)) < 0.5).astype(int) * labels\n",
    "        synthetic_y = np.logical_or(synthetic_y, Y_minority[neighbors[1]]).astype(int)\n",
    "\n",
    "        synthetic_X.append(synthetic_x)\n",
    "        synthetic_Y.append(synthetic_y)\n",
    "    \n",
    "    # Combine original and synthetic data\n",
    "    X_augmented = np.vstack([X, np.array(synthetic_X)])\n",
    "    Y_augmented = np.vstack([Y, np.array(synthetic_Y)])\n",
    "    return X_augmented, Y_augmented\n",
    "\n",
    "# Example usage with your data\n",
    "X_resampled_reaction, Y_resampled_reaction = ml_smote(X_reaction.values, Y_reaction.values, k=5) #, n_samples=1000)\n",
    "\n",
    "print(\"Original data shape:\", X_reaction.shape, Y_reaction.shape)\n",
    "print(\"Resampled data shape:\", X_resampled_reaction.shape, Y_resampled_reaction.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# Address class imbalance using SMOTE\n",
    "#smote = SMOTE()\n",
    "#X_resampled, y_resampled = smote.fit_resample(X, Y)\n",
    "\n",
    "#X_resampled = X.copy()\n",
    "#y_resampled = Y.copy()\n",
    "\n",
    "print(X_resampled_reaction.shape)\n",
    "print(Y_resampled_reaction.shape)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_resampled_reaction = scaler.fit_transform(X_resampled_reaction)\n",
    "\n",
    "print(X_resampled_reaction.shape)\n",
    "print(Y_resampled_reaction.shape)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_reaction, X_test_reaction, y_train_reaction, y_test_reaction = train_test_split(X_resampled_reaction, Y_resampled_reaction, test_size=0.4, random_state=26)\n",
    "\n",
    "X_train_reaction, X_val_reaction, y_train_reaction, y_val_reaction = train_test_split(X_train_reaction, y_train_reaction, test_size=0.2, random_state=26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Count occurrences of 1s for each label (column-wise sum)\n",
    "label_counts = np.sum(Y, axis=0)\n",
    "print(\"Label Counts for Each Label:\", label_counts)\n",
    "\n",
    "# Compute weights inversely proportional to label frequencies\n",
    "weights = 1.0 / label_counts  # Higher weights for minority labels\n",
    "print(\"Sampling Weights for Each Label:\", weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_sums = Y_resampled_reaction.sum(axis=0)\n",
    "print(\"Sum of each column:\", column_sums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.bar(range(len(column_sums)), column_sums)\n",
    "plt.xlabel(\"Labels\")\n",
    "plt.ylabel(\"Occurrences\")\n",
    "plt.title(\"Label Distribution After Resampling\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "!pip install torch -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "# Convert data to torch tensors\n",
    "class Data(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(X.astype(np.float32))\n",
    "        self.y = torch.from_numpy(y.astype(np.float32))  # to_numpy() #Ensure y is a torch tensor\n",
    "        self.len = self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "\n",
    "# Compute sample weights for the training data\n",
    "def compute_sample_weights(y, label_weights):\n",
    "    # Convert label weights (Pandas Series) to NumPy array\n",
    "    label_weights_array = label_weights.values\n",
    "    # Calculate sample weights as the sum of label weights for each row\n",
    "    sample_weights = np.dot(y, label_weights_array) # .to_numpy()\n",
    "    return torch.tensor(sample_weights, dtype=torch.float)\n",
    "\n",
    "\n",
    "# Set batch size\n",
    "batch_size = 512 # 256 # 128 # 64\n",
    "\n",
    "# Instantiate training, validation, and test datasets\n",
    "train_data = Data(X_train_reaction, y_train_reaction)\n",
    "valid_data = Data(X_val_reaction, y_val_reaction)\n",
    "test_data = Data(X_test_reaction, y_test_reaction)\n",
    "\n",
    "# Compute sample weights for the training data\n",
    "sample_weights = compute_sample_weights(y_train_reaction, weights)\n",
    "\n",
    "# Create a WeightedRandomSampler for the training data\n",
    "sampler = WeightedRandomSampler(\n",
    "    weights=sample_weights,\n",
    "    num_samples=len(sample_weights),  # Ensure all samples are considered\n",
    "    replacement=True  # Allows oversampling of minority samples\n",
    ")\n",
    "\n",
    "# Create DataLoaders for training, validation, and test data\n",
    "train_dataloader = DataLoader(dataset=train_data, batch_size=batch_size, sampler=sampler)  # Use sampler for training\n",
    "valid_dataloader = DataLoader(dataset=valid_data, batch_size=batch_size, shuffle=False)   # No sampler for validation\n",
    "test_dataloader = DataLoader(dataset=test_data, batch_size=batch_size, shuffle=False)     # No sampler for testing\n",
    "\n",
    "# Check that it's working\n",
    "for batch, (X, y) in enumerate(train_dataloader):\n",
    "    print(f\"Batch: {batch + 1}\")\n",
    "    print(f\"X shape: {X.shape}\")\n",
    "    print(f\"y shape: {y.shape}\")\n",
    "    break\n",
    "\n",
    "total_label_sums = torch.zeros(y_train_reaction.shape[1])  # Adjust shape based on your dataset\n",
    "for batch_X, batch_Y in train_dataloader:\n",
    "    total_label_sums += batch_Y.sum(axis=0)\n",
    "print(\"Total label distribution across batches:\", total_label_sums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "input_dim = len(feature_cols) #393\n",
    "hidden_dim = 4096 #400\n",
    "output_dim = len(predict_cols)\n",
    "\n",
    "class MultiOutputNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MultiOutputNeuralNetwork, self).__init__()\n",
    "        \n",
    "        # Define multiple layers\n",
    "        self.layer_1 = nn.Linear(input_dim, hidden_dim // 2 + output_dim)\n",
    "        nn.init.kaiming_uniform_(self.layer_1.weight, nonlinearity=\"relu\")\n",
    "        \n",
    "        self.layer_2 = nn.Linear(hidden_dim // 2 + output_dim, hidden_dim // 4 + output_dim)  # Hidden layer (reduce size)\n",
    "        nn.init.kaiming_uniform_(self.layer_2.weight, nonlinearity=\"relu\")\n",
    "        \n",
    "        self.layer_3 = nn.Linear(hidden_dim // 4 + output_dim, hidden_dim // 8 + output_dim)  # Another hidden layer\n",
    "        nn.init.kaiming_uniform_(self.layer_3.weight, nonlinearity=\"relu\")\n",
    "\n",
    "        self.layer_4 = nn.Linear(hidden_dim // 8 + output_dim, hidden_dim // 16 + output_dim)  # Fourth hidden layer\n",
    "        nn.init.kaiming_uniform_(self.layer_4.weight, nonlinearity=\"relu\")\n",
    "        \n",
    "        self.layer_5 = nn.Linear(hidden_dim // 16 + output_dim, hidden_dim // 32 + output_dim)  # Fifth hidden layer\n",
    "        nn.init.kaiming_uniform_(self.layer_5.weight, nonlinearity=\"relu\")\n",
    "\n",
    "        self.layer_6 = nn.Linear(hidden_dim // 32 + output_dim, hidden_dim // 64 + output_dim)  # Fourth hidden layer\n",
    "        nn.init.kaiming_uniform_(self.layer_4.weight, nonlinearity=\"relu\")\n",
    "        \n",
    "        self.layer_7 = nn.Linear(hidden_dim // 64 + output_dim, hidden_dim // 128 + output_dim)  # Fifth hidden layer\n",
    "        nn.init.kaiming_uniform_(self.layer_5.weight, nonlinearity=\"relu\")\n",
    "        \n",
    "        self.output_layer = nn.Linear(hidden_dim // 128 + output_dim, output_dim)  # Final output layer\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.3)  # Define dropout\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pass through layers with ReLU activations\n",
    "        x = torch.nn.functional.relu(self.layer_1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.nn.functional.relu(self.layer_2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.nn.functional.relu(self.layer_3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.nn.functional.relu(self.layer_4(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.nn.functional.relu(self.layer_5(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.nn.functional.relu(self.layer_6(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.nn.functional.relu(self.layer_7(x))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        #x = torch.nn.functional.sigmoid(self.output_layer(x))  # Sigmoid for binary output\n",
    "        #x = torch.sigmoid(self.output_layer(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "       \n",
    "reaction_model = MultiOutputNeuralNetwork(input_dim, hidden_dim, output_dim)\n",
    "print(reaction_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "label_counts = np.sum(y_train_reaction, axis=0)\n",
    "total_samples = y_train_reaction.shape[0]\n",
    "\n",
    "class_weights = total_samples / (len(label_counts) * label_counts)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32)\n",
    "\n",
    "loss_fn = BCEWithLogitsLoss(pos_weight=class_weights)\n",
    "#loss_fn = nn.BCELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(reaction_model.parameters(), lr=learning_rate)\n",
    "\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_epochs = 50\n",
    "train_loss_values = []\n",
    "valid_loss_values = []\n",
    "\n",
    "train_accuracy_values = []\n",
    "valid_accuracy_values = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    reaction_model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "\n",
    "    for X, y in train_dataloader:\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        #print(X.shape)\n",
    "       \n",
    "        # forward pass\n",
    "        pred = reaction_model(X)\n",
    "        loss = loss_fn(pred, y) #.unsqueeze(-1))\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Accuracy calculation\n",
    "        train_correct += (torch.sigmoid(pred) >= 0.5).float().eq(y).sum().item()\n",
    "        #train_correct += (pred >= 0.5).float().eq(y).sum().item()\n",
    "        train_total += y.numel()\n",
    "\n",
    "        # backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # average training loss for the epoch\n",
    "    train_loss /= len(train_dataloader)\n",
    "    train_loss_values.append(train_loss)\n",
    "\n",
    "    # Training accuracy\n",
    "    train_accuracy = train_correct / train_total\n",
    "    train_accuracy_values.append(train_accuracy)\n",
    "\n",
    "    # Validation phase\n",
    "    reaction_model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_val, y_val in valid_dataloader:\n",
    "            pred = reaction_model(X_val)\n",
    "            loss = loss_fn(pred, y_val) #.unsqueeze(-1))\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Accuracy calculation\n",
    "            val_correct += (torch.sigmoid(pred) >= 0.5).float().eq(y_val).sum().item()\n",
    "            #val_correct += (pred >= 0.5).float().eq(y_val).sum().item()\n",
    "            val_total += y_val.numel()\n",
    "    \n",
    "    # Average validation loss for the epoch\n",
    "    val_loss /= len(valid_dataloader)\n",
    "    valid_loss_values.append(val_loss)\n",
    "\n",
    "    # Validation accuracy\n",
    "    val_accuracy = val_correct / val_total\n",
    "    valid_accuracy_values.append(val_accuracy)\n",
    "\n",
    "    # Print loss for the epoch\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "    print(f\"Train Accuracy: {train_accuracy:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "print(\"Training Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze Reaction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Loss and Accuracy\n",
    "epochs = range(1, num_epochs + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Loss plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, train_loss_values, label='Train Loss')\n",
    "plt.plot(epochs, valid_loss_values, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Accuracy plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, train_accuracy_values, label='Train Accuracy')\n",
    "plt.plot(epochs, valid_accuracy_values, label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts = y_train_reaction.sum(axis=0)\n",
    "print(\"Label counts in training data:\", label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, classification_report, hamming_loss\n",
    "from scipy.special import expit  # Sigmoid function\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Dynamically calculate thresholds for all labels\n",
    "def calculate_thresholds(y_true, y_probs):\n",
    "    \"\"\"\n",
    "    Calculate optimal thresholds for each label based on precision-recall curves.\n",
    "    \n",
    "    Parameters:\n",
    "        y_true (array): True binary labels (shape: [n_samples, n_labels]).\n",
    "        y_probs (array): Predicted probabilities (shape: [n_samples, n_labels]).\n",
    "    \n",
    "    Returns:\n",
    "        thresholds_array (array): Optimal thresholds for each label.\n",
    "    \"\"\"\n",
    "    thresholds = []\n",
    "    for i in range(y_true.shape[1]):  # Iterate over all labels dynamically\n",
    "        precision, recall, thresholds_curve = precision_recall_curve(y_true[:, i], y_probs[:, i])\n",
    "        f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)  # Avoid division by zero\n",
    "        optimal_threshold = thresholds_curve[f1_scores.argmax()]  # Find the best threshold\n",
    "        thresholds.append(optimal_threshold)\n",
    "    return np.array(thresholds)\n",
    "\n",
    "# Step 2: Generate predictions using custom thresholds\n",
    "def apply_thresholds(y_probs, thresholds_array):\n",
    "    \"\"\"\n",
    "    Apply custom thresholds to convert probabilities into binary predictions.\n",
    "    \n",
    "    Parameters:\n",
    "        y_probs (array): Predicted probabilities (shape: [n_samples, n_labels]).\n",
    "        thresholds_array (array): Thresholds for each label.\n",
    "    \n",
    "    Returns:\n",
    "        y_pred (array): Binary predictions (shape: [n_samples, n_labels]).\n",
    "    \"\"\"\n",
    "    return (y_probs >= thresholds_array).astype(int)\n",
    "\n",
    "# Use these functions dynamically\n",
    "with torch.no_grad():\n",
    "    y_probs = []  # Collect probabilities\n",
    "    y_true = []  # Collect true labels\n",
    "\n",
    "    for X, y in test_dataloader:\n",
    "        outputs = reaction_model(X)  # Get raw logits\n",
    "        #y_probs.append(outputs.numpy())  # Store probabilities\n",
    "        y_probs.append(expit(outputs.numpy()))  # Convert logits to probabilities\n",
    "        y_true.append(y.numpy())  # Store true labels\n",
    "\n",
    "    # Combine batches into full arrays\n",
    "    y_probs = np.concatenate(y_probs, axis=0)\n",
    "    y_true = np.concatenate(y_true, axis=0)\n",
    "\n",
    "    # Dynamically calculate thresholds\n",
    "    thresholds_array = calculate_thresholds(y_true, y_probs)\n",
    "    print(thresholds_array)\n",
    "\n",
    "    # Apply thresholds to generate binary predictions\n",
    "    y_pred = apply_thresholds(y_probs, thresholds_array)\n",
    "\n",
    "    # Evaluate the model\n",
    "    print(classification_report(y_true, y_pred, target_names=[f\"Label {i}\" for i in range(y_true.shape[1])]))\n",
    "    print(f\"Hamming Loss: {hamming_loss(y_true, y_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools  # Import this at the top of your script\n",
    "\n",
    "# Initialize required variables\n",
    "y_pred = []\n",
    "y_test = []\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "\"\"\"\n",
    "We're not training so we don't need to calculate the gradients for our outputs\n",
    "\"\"\"\n",
    "with torch.no_grad():\n",
    "    for X, y in test_dataloader:\n",
    "        outputs = reaction_model(X)  # Get model outputs\n",
    "        predicted = np.where(outputs.numpy() < 0.5, 0, 1)  # Convert to NumPy and apply threshold\n",
    "        #predicted = list(itertools.chain(*predicted))  # Flatten predictions\n",
    "        y_pred.append(predicted)  # Append predictions\n",
    "        y_test.append(y.numpy())  # Append true labels as NumPy\n",
    "        total += y.size(0)  # Increment total count\n",
    "        correct += (predicted == y.numpy()).sum().item()  # Count correct predictions\n",
    "\n",
    "#print(f'Accuracy of the network on the test instances: {100 * correct // total}%')\n",
    "y_pred = np.concatenate(y_pred, axis=0)  # Combine batches into a single array\n",
    "y_test = np.concatenate(y_test, axis=0)  # Combine batches into a single array\n",
    "total = y_test.size  # Total number of labels\n",
    "correct = (y_pred == y_test).sum()  # Total number of correct predictions\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Accuracy of the network on the test instances: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "\n",
    "# Exclude specific columns\n",
    "columns_to_exclude = ['Case ID', 'Suspect Product Active Ingredients', 'Sex', 'Patient Age', 'Patient Weight', 'Serious']\n",
    "\n",
    "# Create drug-related columns\n",
    "drug_cols = [col for col in df_encoded.columns if col not in columns_to_exclude]\n",
    "\n",
    "# Generate random drug data\n",
    "random_drugs = random.sample(drug_cols, 3)#[\"Insulin Pork\\Insulin Purified Pork\", \"Insulin Beef\"] #random.sample(drug_cols, 3)\n",
    "print(\"Patient Drugs: \", random_drugs)\n",
    "drug_array = np.zeros(len(drug_cols))\n",
    "column_indices = [df_encoded.columns.get_loc(col) for col in random_drugs]\n",
    "for idx in column_indices:\n",
    "    drug_array[idx - 5] = 1\n",
    "\n",
    "# Create a specific patient profile\n",
    "specific_profile = np.array([[1, 83, 65]])  # Adjust profile values as needed\n",
    "specific_profile = np.concatenate([specific_profile, drug_array.reshape(1, -1)], axis=1)\n",
    "\n",
    "# Standardize the profile using the same scaler used during training\n",
    "specific_profile_scaled = scaler.transform(specific_profile)  # 'scaler' is the StandardScaler from training\n",
    "\n",
    "# Convert the profile to a PyTorch tensor\n",
    "specific_profile_tensor = torch.tensor(specific_profile_scaled, dtype=torch.float32)\n",
    "\n",
    "# Predict the outcome using your PyTorch model\n",
    "serious_model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    prediction = serious_model(specific_profile_tensor)\n",
    "    predicted_outcome = (prediction >= 0.5).float().item()  # Threshold of 0.5\n",
    "    predicted_probability = prediction.item()\n",
    "\n",
    "print(f\"Serious Model Prediction: Outcome = {predicted_outcome}, Probability = {predicted_probability:.4f}\")\n",
    "\n",
    "specific_profile_reaction =  torch.cat([specific_profile_tensor, torch.tensor([[predicted_outcome]], dtype=torch.float32)], dim=1)\n",
    "\n",
    "reaction_model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    serious_prediction = reaction_model(specific_profile_reaction)  # Raw logits\n",
    "    #predicted_outcomes = (torch.sigmoid(prediction) >= 0.5).float()  # Apply sigmoid and threshold\n",
    "    serious_predicted_probabilities = torch.sigmoid(serious_prediction).numpy()  # Convert logits to probabilities\n",
    "    #predicted_probabilities = prediction.numpy()  # Convert logits to probabilities\n",
    "    serious_predicted_outcomes = (serious_predicted_probabilities >= thresholds_array).astype(float)  # Apply sigmoid and threshold\n",
    "\n",
    "\n",
    "# Initialize an empty list to store column names with \"Yes\" outcomes and their probabilities\n",
    "yes_labels_with_probabilities = []\n",
    "\n",
    "# Iterate over the predicted outcomes and corresponding probabilities\n",
    "for outcome, probability, column_name in zip(\n",
    "    serious_predicted_outcomes.flatten(), \n",
    "    #prediction.numpy().flatten(), \n",
    "    torch.sigmoid(prediction).numpy().flatten(),\n",
    "    Y_reaction.columns  \n",
    "):\n",
    "    if outcome == 1:  # Check if the outcome is \"Yes\"\n",
    "        # Append column name and probability to the list\n",
    "        yes_labels_with_probabilities.append((column_name, probability))\n",
    "\n",
    "# Print the list of column names with \"Yes\" outcomes and their probabilities\n",
    "print(\"Columns with 'Yes' outcomes and their probabilities:\")\n",
    "for column_name, probability in yes_labels_with_probabilities:\n",
    "    print(f\"{column_name}: Probability = {probability:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
