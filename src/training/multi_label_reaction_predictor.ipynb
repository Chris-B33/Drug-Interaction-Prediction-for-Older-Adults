{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Specify the folder containing your Excel files\n",
    "data_folder = \"Data\"\n",
    "\n",
    "# Define the column names\n",
    "col_names = ['Case ID', 'Suspect Product Active Ingredients', 'Reason for Use', 'Reactions', 'Serious', 'Outcomes', 'Sex', 'Patient Age', 'Patient Weight']\n",
    "\n",
    "# Initialize an empty DataFrame to store the combined data\n",
    "combined_data = pd.DataFrame(columns=col_names)\n",
    "\n",
    "# Iterate over all Excel files in the folder\n",
    "for file in os.listdir(data_folder):\n",
    "    if file.endswith(\".xlsx\"):  # Check if the file is an Excel file\n",
    "        file_path = os.path.join(data_folder, file)\n",
    "        # Read the Excel file and add it to the combined DataFrame\n",
    "        data = pd.read_excel(file_path, usecols=col_names)  # Load only the specified columns\n",
    "        combined_data = pd.concat([combined_data, data], ignore_index=True)\n",
    "\n",
    "# Display the combined dataset\n",
    "print(combined_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = ['Case ID', 'Suspect Product Active Ingredients', 'Reason for Use', 'Reactions', 'Serious', 'Outcomes', 'Sex', 'Patient Age', 'Patient Weight']\n",
    "\n",
    "# load dataset\n",
    "data = pd.read_excel(\"Data\\\\INSULIN BEEF (G).xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = ['Case ID', 'Suspect Product Active Ingredients', 'Reactions',  'Serious', 'Sex', 'Patient Age', 'Patient Weight']\n",
    "\n",
    "#df = data[selected_columns]\n",
    "df = combined_data[selected_columns]\n",
    "\n",
    "df.head()\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Suspect Product Active Ingredients'] = df['Suspect Product Active Ingredients'].str.split(';')\n",
    "df_split_temp = df.explode('Suspect Product Active Ingredients', ignore_index=True)\n",
    "\n",
    "df_split_temp['Reactions'] = df_split_temp['Reactions'].str.split(';')\n",
    "df_split = df_split_temp.explode('Reactions', ignore_index = True)\n",
    "\n",
    "df_split.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = df_split['Serious'].value_counts()\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_split.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_multi = pd.get_dummies(df_split, columns = ['Suspect Product Active Ingredients', 'Reactions'], prefix=['Product', 'Reaction'], prefix_sep='_')\n",
    "reactions = [col for col in df_multi.columns if col.startswith(\"Reaction_\")]\n",
    "df_reaction = df_multi[reactions]\n",
    "df_multi = df_multi.groupby('Case ID').max().reset_index()\n",
    "\n",
    "columns_to_exclude = ['Case ID', 'Suspect Product Active Ingredients', 'Reactions', 'Serious', 'Sex', 'Patient Age', 'Patient Weight']\n",
    "\n",
    "columns_to_convert = [col for col in df_multi.columns if col.startswith(\"Product_\") or col.startswith(\"Reaction_\")] #not in columns_to_exclude]\n",
    "\n",
    "df_multi[columns_to_convert] = df_multi[columns_to_convert].astype(int)\n",
    "\n",
    "df_final = df_multi\n",
    "print(df_multi.columns)\n",
    "print(df_final)\n",
    "\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_reaction.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "react = [col.replace(\"Reaction_\", \"\") for col in df_reaction.columns]\n",
    "\n",
    "for r in react:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Group reactions by the first word\n",
    "grouped_reactions = defaultdict(list)\n",
    "\n",
    "for r in react:\n",
    "    # Split reaction into words and extract the first word\n",
    "    first_word = r.split()[0]\n",
    "    # Group reactions by the first word\n",
    "    grouped_reactions[first_word].append(r)\n",
    "i = 0\n",
    "# Print the grouped reactions\n",
    "for word, group in grouped_reactions.items():\n",
    "    i = i + 1\n",
    "    print(f\"{word}: {group}\")\n",
    "\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded = df_multi.copy()\n",
    "print(df_encoded.columns)\n",
    "#print(df_split['Patient Age'])\n",
    "df_encoded['Patient Age'] = df_encoded['Patient Age'].astype(str)\n",
    "df_encoded['Patient Age'] = df_encoded['Patient Age'].str.replace(r'\\D+', '', regex=True)\n",
    "df_encoded['Patient Age'] = pd.to_numeric(df_encoded['Patient Age'], errors='coerce')  # Converts to numeric, sets invalid values to NaN\n",
    "\n",
    "df_encoded['Patient Weight'] = df_encoded['Patient Weight'].replace('Not Specified', \"0 KG\")\n",
    "df_encoded['Patient Weight'] = df_encoded['Patient Weight'].astype(str)\n",
    "df_encoded['Patient Weight'] = df_encoded['Patient Weight'].str.replace(r'[^\\d.]', '', regex=True)\n",
    "df_encoded['Patient Weight'] = pd.to_numeric(df_encoded['Patient Weight'], errors='coerce')\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "#df_encoded['Suspect Product Active Ingredients'] = label_encoder.fit_transform(df_encoded['Suspect Product Active Ingredients'])\n",
    "df_encoded['Sex'] = label_encoder.fit_transform(df_encoded['Sex'])\n",
    "df_encoded['Serious'] = label_encoder.fit_transform(df_encoded['Serious'])\n",
    "print(df_encoded.isnull().sum())\n",
    "df_encoded.dropna(inplace=True)\n",
    "print(df_encoded.isnull().sum())\n",
    "df_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = df_encoded['Serious'].value_counts()\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum columns that start with \"Reaction_\"\n",
    "reaction_columns = df_encoded.filter(like=\"Reaction_\", axis=1)\n",
    "column_sums = reaction_columns.sum(axis=0)\n",
    "\n",
    "print(len(reaction_columns.columns))\n",
    "# Set the threshold\n",
    "threshold = 50 #300\n",
    "\n",
    "# Filter column names with sums greater than the threshold\n",
    "columns_above_threshold = column_sums[column_sums > threshold].index.tolist()\n",
    "\n",
    "print(columns_above_threshold)\n",
    "print(len(columns_above_threshold))\n",
    "\n",
    "filtered_df = df_encoded[df_encoded[columns_above_threshold].sum(axis=1) > 0]\n",
    "\n",
    "# Print the number of rows before and after filtering\n",
    "print(f\"Number of rows before filtering: {df_encoded.shape[0]}\")\n",
    "print(f\"Number of rows after removing all-zero rows: {filtered_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_exclude = ['Case ID', 'Suspect Product Active Ingredients', 'Reactions']#, 'Serious']\n",
    "\n",
    "feature_cols = [col for col in df_encoded.columns if col not in cols_to_exclude and not col.startswith(\"Reaction_\")]\n",
    "\n",
    "print(feature_cols)\n",
    "print(len(feature_cols))\n",
    "\n",
    "predict_cols = []#[\"Serious\"]\n",
    "reaction_cols = columns_above_threshold #[col for col in df_encoded.columns if col.startswith(\"Reaction_\")]\n",
    "predict_cols.extend(reaction_cols)\n",
    "\n",
    "print(predict_cols)\n",
    "print(len(predict_cols))\n",
    "\n",
    "df_filtered = df_encoded[df_encoded[predict_cols].sum(axis=1) > 0]\n",
    "\n",
    "X = df_filtered[feature_cols]\n",
    "Y = df_filtered[predict_cols]\n",
    "\n",
    "#X = df_encoded[feature_cols]\n",
    "#Y = df_encoded[predict_cols] #df_encoded.Serious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming X is a Pandas DataFrame\n",
    "column_totals = Y.sum()  # Sums each column\n",
    "for column, total in column_totals.items():\n",
    "    print(f\"Column: {column}, Total: {total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "\n",
    "def ml_smote(X, Y, k=5, target_samples=None):\n",
    "    \"\"\"\n",
    "    MLSMOTE implementation for multi-label datasets.\n",
    "    \n",
    "    Parameters:\n",
    "        X (array): Feature matrix (NumPy array or DataFrame).\n",
    "        Y (array): Multi-label matrix (NumPy array or DataFrame).\n",
    "        k (int): Number of nearest neighbors.\n",
    "        n_samples (int): Number of synthetic samples to generate.\n",
    "    \n",
    "    Returns:\n",
    "        X_augmented, Y_augmented: Augmented feature and label matrices.\n",
    "    \"\"\"\n",
    "    # Calculate imbalance ratio per label (IRPL)\n",
    "    label_frequencies = Y.sum(axis=0)\n",
    "    irpl = max(label_frequencies) / label_frequencies\n",
    "    \n",
    "    # Calculate mean imbalance ratio (MIR)\n",
    "    mir = np.mean(irpl)\n",
    "    \n",
    "    # Identify tail labels (labels with IRPL > MIR)\n",
    "    tail_labels = np.where(irpl > mir)[0]\n",
    "    print(tail_labels)\n",
    "    \n",
    "    # Identify minority samples\n",
    "    #minority_indices = np.where(Y.sum(axis=1) < Y.shape[1] / 2)[0]  # Example threshold\n",
    "    minority_indices = np.where((Y[:, tail_labels].sum(axis=1)) > 0)[0]\n",
    "    X_minority = X[minority_indices]\n",
    "    Y_minority = Y[minority_indices]\n",
    "    \n",
    "    # Fit nearest neighbors\n",
    "    knn = NearestNeighbors(n_neighbors=k).fit(X_minority)\n",
    "    synthetic_X, synthetic_Y = [], []\n",
    "\n",
    "    # Calculate target samples for each tail label\n",
    "    if target_samples is None:\n",
    "        target_samples = (label_frequencies.max() - label_frequencies).astype(int)\n",
    "\n",
    "    #for _ in range(n_samples):\n",
    "    for _ in range(target_samples.sum()):\n",
    "        # Randomly select a minority sample\n",
    "        idx = np.random.choice(len(X_minority))\n",
    "        x = X_minority[idx]\n",
    "        labels = Y_minority[idx]\n",
    "\n",
    "        # Find k-nearest neighbors\n",
    "        neighbors = knn.kneighbors([x], return_distance=False)[0]\n",
    "        #neighbor_idx = np.random.choice(neighbors[1:])  # Avoid selecting the sample itself\n",
    "        selected_neighbors = neighbors[1:]  # Exclude itself\n",
    "\n",
    "        # Interpolate between the sample and a neighbor\n",
    "        #neighbor_x = X_minority[neighbor_idx]\n",
    "        #synthetic_x = x + np.random.rand() * (neighbor_x - x)\n",
    "        #synthetic_y = np.logical_or(labels, Y_minority[neighbor_idx]).astype(int)\n",
    "\n",
    "        # Generate synthetic features by averaging neighbors\n",
    "        weights = np.random.dirichlet(np.ones(len(selected_neighbors)))\n",
    "        neighbor_x = np.dot(weights, X_minority[selected_neighbors])\n",
    "        synthetic_x = x + np.random.rand() * (neighbor_x - x)\n",
    "\n",
    "        # Generate synthetic labels probabilistically\n",
    "        synthetic_y = (np.random.rand(len(labels)) < 0.5).astype(int) * labels\n",
    "        synthetic_y = np.logical_or(synthetic_y, Y_minority[neighbors[1]]).astype(int)\n",
    "\n",
    "        synthetic_X.append(synthetic_x)\n",
    "        synthetic_Y.append(synthetic_y)\n",
    "    \n",
    "    # Combine original and synthetic data\n",
    "    X_augmented = np.vstack([X, np.array(synthetic_X)])\n",
    "    Y_augmented = np.vstack([Y, np.array(synthetic_Y)])\n",
    "    return X_augmented, Y_augmented\n",
    "\n",
    "# Example usage with your data\n",
    "X_resampled, Y_resampled = ml_smote(X.values, Y.values, k=5) #, n_samples=1000)\n",
    "\n",
    "print(\"Original data shape:\", X.shape, Y.shape)\n",
    "print(\"Resampled data shape:\", X_resampled.shape, Y_resampled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# Address class imbalance using SMOTE\n",
    "#smote = SMOTE()\n",
    "#X_resampled, y_resampled = smote.fit_resample(X, Y)\n",
    "\n",
    "#X_resampled = X.copy()\n",
    "#y_resampled = Y.copy()\n",
    "\n",
    "print(X_resampled.shape)\n",
    "print(Y_resampled.shape)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_resampled = scaler.fit_transform(X_resampled)\n",
    "\n",
    "print(X_resampled.shape)\n",
    "print(Y_resampled.shape)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, Y_resampled, test_size=0.4, random_state=26)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=26)\n",
    "\n",
    "#!pip install imbalanced-learn\n",
    "\n",
    "#from imblearn.over_sampling import SMOTE\n",
    "\n",
    "#smote = SMOTE(random_state=42)\n",
    "#X_resampled, y_resampled = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Count occurrences of 1s for each label (column-wise sum)\n",
    "label_counts = np.sum(Y, axis=0)\n",
    "print(\"Label Counts for Each Label:\", label_counts)\n",
    "\n",
    "# Compute weights inversely proportional to label frequencies\n",
    "weights = 1.0 / label_counts  # Higher weights for minority labels\n",
    "print(\"Sampling Weights for Each Label:\", weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "# Convert sample weights to a PyTorch tensor\n",
    "sample_weights_tensor = torch.tensor(sample_weights, dtype=torch.float)\n",
    "\n",
    "# Create a WeightedRandomSampler\n",
    "sampler = WeightedRandomSampler(\n",
    "    weights=sample_weights_tensor,\n",
    "    num_samples=len(sample_weights_tensor),  # Keep the number of samples the same as your dataset\n",
    "    replacement=True  # Allows oversampling of minority samples\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_sums = Y_resampled.sum(axis=0)\n",
    "print(\"Sum of each column:\", column_sums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.bar(range(len(column_sums)), column_sums)\n",
    "plt.xlabel(\"Labels\")\n",
    "plt.ylabel(\"Occurrences\")\n",
    "plt.title(\"Label Distribution After Resampling\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for column in Y_resampled.columns:\n",
    "#    print(f\"Label: {column}, Distribution: {Y_resampled[column].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "!pip install torch -q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#import torch\n",
    "#import numpy as np\n",
    "#from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Convert data to torch tensors\n",
    "#class Data(Dataset):\n",
    "#    def __init__(self, X, y):\n",
    "        #self.X = torch.from_numpy(X.to_numpy().astype(np.float32))\n",
    "        #self.y = torch.from_numpy(y.to_numpy().astype(np.float32))\n",
    "#        self.X = torch.from_numpy(X.astype(np.float32))\n",
    "#        self.y = torch.from_numpy(y.to_numpy().astype(np.float32))\n",
    "#        self.len = self.X.shape[0]\n",
    "       \n",
    "#    def __getitem__(self, index):\n",
    "#        return self.X[index], self.y[index]\n",
    "   \n",
    "#    def __len__(self):\n",
    "#        return self.len\n",
    "   \n",
    "#batch_size = 64\n",
    "\n",
    "# Instantiate training and test data\n",
    "#train_data = Data(X_train, y_train)\n",
    "#train_data = Data(X_resampled, y_resampled)\n",
    "#train_dataloader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#test_data = Data(X_test, y_test)\n",
    "#test_dataloader = DataLoader(dataset=test_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#valid_data = Data(X_val, y_val)\n",
    "#valid_dataloader = DataLoader(dataset=valid_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Check it's working\n",
    "#for batch, (X, y) in enumerate(train_dataloader):\n",
    "#    print(f\"Batch: {batch+1}\")\n",
    "#    print(f\"X shape: {X.shape}\")\n",
    "#    print(f\"y shape: {y.shape}\")\n",
    "#    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "# Convert data to torch tensors\n",
    "class Data(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(X.astype(np.float32))\n",
    "        self.y = torch.from_numpy(y.astype(np.float32))  # to_numpy() #Ensure y is a torch tensor\n",
    "        self.len = self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "\n",
    "# Compute sample weights for the training data\n",
    "def compute_sample_weights(y, label_weights):\n",
    "    # Convert label weights (Pandas Series) to NumPy array\n",
    "    label_weights_array = label_weights.values\n",
    "    # Calculate sample weights as the sum of label weights for each row\n",
    "    sample_weights = np.dot(y, label_weights_array) # .to_numpy()\n",
    "    return torch.tensor(sample_weights, dtype=torch.float)\n",
    "\n",
    "\n",
    "# Set batch size\n",
    "batch_size = 512 # 256 # 128 # 64\n",
    "\n",
    "# Instantiate training, validation, and test datasets\n",
    "train_data = Data(X_train, y_train)\n",
    "valid_data = Data(X_val, y_val)\n",
    "test_data = Data(X_test, y_test)\n",
    "\n",
    "# Compute sample weights for the training data\n",
    "sample_weights = compute_sample_weights(y_train, weights)\n",
    "\n",
    "# Create a WeightedRandomSampler for the training data\n",
    "sampler = WeightedRandomSampler(\n",
    "    weights=sample_weights,\n",
    "    num_samples=len(sample_weights),  # Ensure all samples are considered\n",
    "    replacement=True  # Allows oversampling of minority samples\n",
    ")\n",
    "\n",
    "# Create DataLoaders for training, validation, and test data\n",
    "train_dataloader = DataLoader(dataset=train_data, batch_size=batch_size, sampler=sampler)  # Use sampler for training\n",
    "valid_dataloader = DataLoader(dataset=valid_data, batch_size=batch_size, shuffle=False)   # No sampler for validation\n",
    "test_dataloader = DataLoader(dataset=test_data, batch_size=batch_size, shuffle=False)     # No sampler for testing\n",
    "\n",
    "# Check that it's working\n",
    "for batch, (X, y) in enumerate(train_dataloader):\n",
    "    print(f\"Batch: {batch + 1}\")\n",
    "    print(f\"X shape: {X.shape}\")\n",
    "    print(f\"y shape: {y.shape}\")\n",
    "    break\n",
    "\n",
    "total_label_sums = torch.zeros(y_train.shape[1])  # Adjust shape based on your dataset\n",
    "for batch_X, batch_Y in train_dataloader:\n",
    "    total_label_sums += batch_Y.sum(axis=0)\n",
    "print(\"Total label distribution across batches:\", total_label_sums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "input_dim = len(feature_cols) #393\n",
    "hidden_dim = 4096 #400\n",
    "output_dim = len(predict_cols)\n",
    "\n",
    "class MultiOutputNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MultiOutputNeuralNetwork, self).__init__()\n",
    "        \n",
    "        # Define multiple layers\n",
    "        self.layer_1 = nn.Linear(input_dim, hidden_dim // 2 + output_dim)\n",
    "        nn.init.kaiming_uniform_(self.layer_1.weight, nonlinearity=\"relu\")\n",
    "        \n",
    "        self.layer_2 = nn.Linear(hidden_dim // 2 + output_dim, hidden_dim // 4 + output_dim)  # Hidden layer (reduce size)\n",
    "        nn.init.kaiming_uniform_(self.layer_2.weight, nonlinearity=\"relu\")\n",
    "        \n",
    "        self.layer_3 = nn.Linear(hidden_dim // 4 + output_dim, hidden_dim // 8 + output_dim)  # Another hidden layer\n",
    "        nn.init.kaiming_uniform_(self.layer_3.weight, nonlinearity=\"relu\")\n",
    "\n",
    "        self.layer_4 = nn.Linear(hidden_dim // 8 + output_dim, hidden_dim // 16 + output_dim)  # Fourth hidden layer\n",
    "        nn.init.kaiming_uniform_(self.layer_4.weight, nonlinearity=\"relu\")\n",
    "        \n",
    "        self.layer_5 = nn.Linear(hidden_dim // 16 + output_dim, hidden_dim // 32 + output_dim)  # Fifth hidden layer\n",
    "        nn.init.kaiming_uniform_(self.layer_5.weight, nonlinearity=\"relu\")\n",
    "\n",
    "        self.layer_6 = nn.Linear(hidden_dim // 32 + output_dim, hidden_dim // 64 + output_dim)  # Fourth hidden layer\n",
    "        nn.init.kaiming_uniform_(self.layer_4.weight, nonlinearity=\"relu\")\n",
    "        \n",
    "        self.layer_7 = nn.Linear(hidden_dim // 64 + output_dim, hidden_dim // 128 + output_dim)  # Fifth hidden layer\n",
    "        nn.init.kaiming_uniform_(self.layer_5.weight, nonlinearity=\"relu\")\n",
    "        \n",
    "        self.output_layer = nn.Linear(hidden_dim // 128 + output_dim, output_dim)  # Final output layer\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.3)  # Define dropout\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pass through layers with ReLU activations\n",
    "        x = torch.nn.functional.relu(self.layer_1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.nn.functional.relu(self.layer_2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.nn.functional.relu(self.layer_3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.nn.functional.relu(self.layer_4(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.nn.functional.relu(self.layer_5(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.nn.functional.relu(self.layer_6(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.nn.functional.relu(self.layer_7(x))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        #x = torch.nn.functional.sigmoid(self.output_layer(x))  # Sigmoid for binary output\n",
    "        #x = torch.sigmoid(self.output_layer(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "       \n",
    "model = MultiOutputNeuralNetwork(input_dim, hidden_dim, output_dim)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "label_counts = np.sum(y_train, axis=0)\n",
    "total_samples = y_train.shape[0]\n",
    "\n",
    "class_weights = total_samples / (len(label_counts) * label_counts)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32)\n",
    "\n",
    "loss_fn = BCEWithLogitsLoss(pos_weight=class_weights)\n",
    "#loss_fn = nn.BCELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_epochs = 50\n",
    "train_loss_values = []\n",
    "valid_loss_values = []\n",
    "\n",
    "train_accuracy_values = []\n",
    "valid_accuracy_values = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "\n",
    "    for X, y in train_dataloader:\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        #print(X.shape)\n",
    "       \n",
    "        # forward pass\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y) #.unsqueeze(-1))\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Accuracy calculation\n",
    "        train_correct += (torch.sigmoid(pred) >= 0.5).float().eq(y).sum().item()\n",
    "        #train_correct += (pred >= 0.5).float().eq(y).sum().item()\n",
    "        train_total += y.numel()\n",
    "\n",
    "        # backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # average training loss for the epoch\n",
    "    train_loss /= len(train_dataloader)\n",
    "    train_loss_values.append(train_loss)\n",
    "\n",
    "    # Training accuracy\n",
    "    train_accuracy = train_correct / train_total\n",
    "    train_accuracy_values.append(train_accuracy)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_val, y_val in valid_dataloader:\n",
    "            pred = model(X_val)\n",
    "            loss = loss_fn(pred, y_val) #.unsqueeze(-1))\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Accuracy calculation\n",
    "            val_correct += (torch.sigmoid(pred) >= 0.5).float().eq(y_val).sum().item()\n",
    "            #val_correct += (pred >= 0.5).float().eq(y_val).sum().item()\n",
    "            val_total += y_val.numel()\n",
    "    \n",
    "    # Average validation loss for the epoch\n",
    "    val_loss /= len(valid_dataloader)\n",
    "    valid_loss_values.append(val_loss)\n",
    "\n",
    "    # Validation accuracy\n",
    "    val_accuracy = val_correct / val_total\n",
    "    valid_accuracy_values.append(val_accuracy)\n",
    "\n",
    "    # Print loss for the epoch\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "    print(f\"Train Accuracy: {train_accuracy:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "print(\"Training Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Loss and Accuracy\n",
    "epochs = range(1, num_epochs + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Loss plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, train_loss_values, label='Train Loss')\n",
    "plt.plot(epochs, valid_loss_values, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Accuracy plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, train_accuracy_values, label='Train Accuracy')\n",
    "plt.plot(epochs, valid_accuracy_values, label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts = y_train.sum(axis=0)\n",
    "print(\"Label counts in training data:\", label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(1, num_epochs + 1), train_loss_values, label='Train Loss')\n",
    "plt.plot(range(1, num_epochs + 1), valid_loss_values, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, classification_report, hamming_loss\n",
    "from scipy.special import expit  # Sigmoid function\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Dynamically calculate thresholds for all labels\n",
    "def calculate_thresholds(y_true, y_probs):\n",
    "    \"\"\"\n",
    "    Calculate optimal thresholds for each label based on precision-recall curves.\n",
    "    \n",
    "    Parameters:\n",
    "        y_true (array): True binary labels (shape: [n_samples, n_labels]).\n",
    "        y_probs (array): Predicted probabilities (shape: [n_samples, n_labels]).\n",
    "    \n",
    "    Returns:\n",
    "        thresholds_array (array): Optimal thresholds for each label.\n",
    "    \"\"\"\n",
    "    thresholds = []\n",
    "    for i in range(y_true.shape[1]):  # Iterate over all labels dynamically\n",
    "        precision, recall, thresholds_curve = precision_recall_curve(y_true[:, i], y_probs[:, i])\n",
    "        f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)  # Avoid division by zero\n",
    "        optimal_threshold = thresholds_curve[f1_scores.argmax()]  # Find the best threshold\n",
    "        thresholds.append(optimal_threshold)\n",
    "    return np.array(thresholds)\n",
    "\n",
    "# Step 2: Generate predictions using custom thresholds\n",
    "def apply_thresholds(y_probs, thresholds_array):\n",
    "    \"\"\"\n",
    "    Apply custom thresholds to convert probabilities into binary predictions.\n",
    "    \n",
    "    Parameters:\n",
    "        y_probs (array): Predicted probabilities (shape: [n_samples, n_labels]).\n",
    "        thresholds_array (array): Thresholds for each label.\n",
    "    \n",
    "    Returns:\n",
    "        y_pred (array): Binary predictions (shape: [n_samples, n_labels]).\n",
    "    \"\"\"\n",
    "    return (y_probs >= thresholds_array).astype(int)\n",
    "\n",
    "# Use these functions dynamically\n",
    "with torch.no_grad():\n",
    "    y_probs = []  # Collect probabilities\n",
    "    y_true = []  # Collect true labels\n",
    "\n",
    "    for X, y in test_dataloader:\n",
    "        outputs = model(X)  # Get raw logits\n",
    "        #y_probs.append(outputs.numpy())  # Store probabilities\n",
    "        y_probs.append(expit(outputs.numpy()))  # Convert logits to probabilities\n",
    "        y_true.append(y.numpy())  # Store true labels\n",
    "\n",
    "    # Combine batches into full arrays\n",
    "    y_probs = np.concatenate(y_probs, axis=0)\n",
    "    y_true = np.concatenate(y_true, axis=0)\n",
    "\n",
    "    # Dynamically calculate thresholds\n",
    "    thresholds_array = calculate_thresholds(y_true, y_probs)\n",
    "    print(thresholds_array)\n",
    "\n",
    "    # Apply thresholds to generate binary predictions\n",
    "    y_pred = apply_thresholds(y_probs, thresholds_array)\n",
    "\n",
    "    # Evaluate the model\n",
    "    print(classification_report(y_true, y_pred, target_names=[f\"Label {i}\" for i in range(y_true.shape[1])]))\n",
    "    print(f\"Hamming Loss: {hamming_loss(y_true, y_pred):.4f}\")\n",
    "\n",
    "    # [0.47408065 0.52283853 0.21895869 0.27320343 0.6131553  0.46514297\n",
    " #0.47177306 0.4537501  0.3889547  0.44084015 0.37250233 0.672361\n",
    " #0.32849404 0.55169785 0.35388562 0.60575116 0.28624716 0.3597172\n",
    " #0.34597415 0.3506796  0.53688514 0.31747028 0.77734935 0.31698427\n",
    " #0.24684684 0.3295522  0.22854547 0.3962895  0.25093877 0.3283029\n",
    " #0.4727016  0.29456106 0.26963988 0.13329153 0.39464808 0.19385336\n",
    " #0.3715566  0.11622908 0.1728393  0.50060636 0.13731995 0.24483377\n",
    " #0.3754116  0.31347966 0.34662092 0.33132926 0.25891978 0.21420503\n",
    " #0.2166132  0.35917073 0.46442243 0.5035977  0.11324815 0.33370334\n",
    " #0.30403858 0.7453642  0.30300874 0.5743012  0.25913933 0.76659316\n",
    " #0.3750667  0.17950211 0.30545366 0.18148132 0.50782764 0.24113299\n",
    "# 0.2444831  0.20651104 0.33514607 0.2554716  0.2989948  0.43072158\n",
    " #0.47554058 0.35983965 0.42703182 0.1700732  0.1999757  0.63880205\n",
    " #0.31359017 0.20798978 0.5765725  0.2324845  0.57795274 0.5981137\n",
    " #0.2000306  0.34174556 0.21019131 0.2839584  0.26304948 0.462434\n",
    " #0.2133022  0.12184759 0.34505197 0.48796624 0.90137494 0.5976198\n",
    " #0.26096636 0.7713822  0.6936902  0.7387786  0.19493434 0.5385473\n",
    " #0.08584324 0.23921402 0.47752047 0.2028372  0.17308132 0.21509966\n",
    " #0.24524291 0.84455794 0.4313751  0.5592498  0.66809034 0.40178862\n",
    " #0.2039171  0.21381447 0.8629443  0.487138   0.6246119  0.7029666\n",
    " #0.411958   0.27293777 0.23883899 0.25088048 0.56650364 0.50837135\n",
    " #0.11624123 0.424044   0.24882652 0.5561839  0.4825503  0.21246974\n",
    " #0.33677018 0.3970621  0.31884047 0.31590977 0.54259104 0.41429722\n",
    " #0.35419166]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools  # Import this at the top of your script\n",
    "\n",
    "# Initialize required variables\n",
    "y_pred = []\n",
    "y_test = []\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "\"\"\"\n",
    "We're not training so we don't need to calculate the gradients for our outputs\n",
    "\"\"\"\n",
    "with torch.no_grad():\n",
    "    for X, y in test_dataloader:\n",
    "        outputs = model(X)  # Get model outputs\n",
    "        predicted = np.where(outputs.numpy() < 0.5, 0, 1)  # Convert to NumPy and apply threshold\n",
    "        #predicted = list(itertools.chain(*predicted))  # Flatten predictions\n",
    "        y_pred.append(predicted)  # Append predictions\n",
    "        y_test.append(y.numpy())  # Append true labels as NumPy\n",
    "        total += y.size(0)  # Increment total count\n",
    "        correct += (predicted == y.numpy()).sum().item()  # Count correct predictions\n",
    "\n",
    "#print(f'Accuracy of the network on the test instances: {100 * correct // total}%')\n",
    "y_pred = np.concatenate(y_pred, axis=0)  # Combine batches into a single array\n",
    "y_test = np.concatenate(y_test, axis=0)  # Combine batches into a single array\n",
    "total = y_test.size  # Total number of labels\n",
    "correct = (y_pred == y_test).sum()  # Total number of correct predictions\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Accuracy of the network on the test instances: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "#y_pred = list(itertools.chain(*y_pred))\n",
    "#y_test = list(itertools.chain(*y_test))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import hamming_loss, accuracy_score\n",
    "\n",
    "# Flatten labels if necessary for multilabel\n",
    "y_pred_flat = np.array(y_pred).reshape(-1)\n",
    "y_test_flat = np.array(y_test).reshape(-1)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Combine batches of predictions and true labels into a single array\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "# Generate predictions for all validation batches\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for X_val, y_val in valid_dataloader:\n",
    "        preds = torch.sigmoid(model(X_val))  # Apply sigmoid to get probabilities\n",
    "        preds = (preds > 0.5).int()  # Convert probabilities to binary predictions\n",
    "        y_pred.append(preds.numpy())\n",
    "        y_true.append(y_val.numpy())\n",
    "\n",
    "# Flatten predictions and true labels\n",
    "y_pred = np.concatenate(y_pred, axis=0)\n",
    "y_true = np.concatenate(y_true, axis=0)\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(y_true, y_pred, target_names=predict_cols, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import hamming_loss, f1_score\n",
    "\n",
    "# Example for F1-score\n",
    "#y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "y_pred_binary = (y_pred > thresholds_array).astype(int)\n",
    "print(\"Hamming Loss:\", hamming_loss(y_true, y_pred_binary))\n",
    "print(\"Micro F1-score:\", f1_score(y_true, y_pred_binary, average='micro'))\n",
    "print(\"Macro F1-score:\", f1_score(y_true, y_pred_binary, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cf_matrix = confusion_matrix(y_test, y_pred)\n",
    "#cf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(thresholds_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "\n",
    "# Exclude specific columns\n",
    "columns_to_exclude = ['Case ID', 'Suspect Product Active Ingredients', 'Sex', 'Patient Age', 'Patient Weight', 'Serious', 'Reactions']\n",
    "\n",
    "# Create drug-related columns\n",
    "drug_cols = [col for col in df_encoded.columns if col.startswith(\"Product_\")] #not in columns_to_exclude]\n",
    "\n",
    "# Generate random drug data\n",
    "random_drugs = random.sample(drug_cols, 5)#[\"Insulin Pork\\Insulin Purified Pork\", \"Insulin Beef\"] #random.sample(drug_cols, 3)\n",
    "print(\"Patient Drugs: \", random_drugs)\n",
    "drug_array = np.zeros(len(drug_cols))\n",
    "column_indices = [df_encoded.columns.get_loc(col) for col in random_drugs]\n",
    "for idx in column_indices:\n",
    "    drug_array[idx - 5] = 1  # Adjust index as needed\n",
    "\n",
    "# Create a specific patient profile\n",
    "specific_profile = np.array([[1, 1, 83, 74]])  # Adjust profile values as needed\n",
    "specific_profile = np.concatenate([specific_profile, drug_array.reshape(1, -1)], axis=1)\n",
    "\n",
    "# Ensure the feature order matches the training data\n",
    "#columns_to_include = [col for col in X.columns if col not in columns_to_exclude]  # 'X' should be your training feature matrix\n",
    "#specific_profile = specific_profile[:, :len(columns_to_include)]  # Align shape with training features\n",
    "\n",
    "# Standardize the profile using the same scaler used during training\n",
    "specific_profile_scaled = specific_profile #scaler.transform(specific_profile)  # 'scaler' is the StandardScaler from training\n",
    "\n",
    "# Convert the profile to a PyTorch tensor\n",
    "specific_profile_tensor = torch.tensor(specific_profile_scaled, dtype=torch.float32)\n",
    "\n",
    "# Predict the outcome using your PyTorch model\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    prediction = model(specific_profile_tensor)  # Raw logits\n",
    "    #predicted_outcomes = (torch.sigmoid(prediction) >= 0.5).float()  # Apply sigmoid and threshold\n",
    "    predicted_probabilities = torch.sigmoid(prediction).numpy()  # Convert logits to probabilities\n",
    "    #predicted_probabilities = prediction.numpy()  # Convert logits to probabilities\n",
    "    predicted_outcomes = (predicted_probabilities >= thresholds_array).astype(float)  # Apply sigmoid and threshold\n",
    "\n",
    "# Output the results for generic labels\n",
    "#for i, (outcome, probability) in enumerate(zip(predicted_outcomes.flatten(), predicted_probabilities.flatten()), 1):\n",
    "#    print(f\"Label {i}:\")\n",
    "#    print(f\"  Occurrence: {'Yes' if outcome == 1 else 'No'}\")\n",
    "#    print(f\"  Probability: {probability:.4f}\")\n",
    "\n",
    "# Initialize an empty list to store labels with \"Yes\" outcomes and their probabilities\n",
    "#yes_labels_with_probabilities = []\n",
    "\n",
    "# Iterate over the predicted outcomes and corresponding probabilities\n",
    "#for i, (outcome, probability) in enumerate(zip(predicted_outcomes.flatten(), prediction.numpy().flatten()), 1):\n",
    "#    if outcome == 1:  # Check if the outcome is \"Yes\"\n",
    "        # Append label and probability to the list\n",
    "#        yes_labels_with_probabilities.append((f\"Label {i}\", probability))\n",
    "\n",
    "# Print the list of labels with \"Yes\" outcomes and their probabilities\n",
    "#print(\"Labels with 'Yes' outcomes and their probabilities:\")\n",
    "#for label, probability in yes_labels_with_probabilities:\n",
    "#    print(f\"{label}: Probability = {probability:.4f}\")\n",
    "\n",
    "# Initialize an empty list to store column names with \"Yes\" outcomes and their probabilities\n",
    "yes_labels_with_probabilities = []\n",
    "\n",
    "# Iterate over the predicted outcomes and corresponding probabilities\n",
    "for outcome, probability, column_name in zip(\n",
    "    predicted_outcomes.flatten(), \n",
    "    #prediction.numpy().flatten(), \n",
    "    torch.sigmoid(prediction).numpy().flatten(),\n",
    "    Y.columns  \n",
    "):\n",
    "    if outcome == 1:  # Check if the outcome is \"Yes\"\n",
    "        # Append column name and probability to the list\n",
    "        yes_labels_with_probabilities.append((column_name, probability))\n",
    "\n",
    "# Print the list of column names with \"Yes\" outcomes and their probabilities\n",
    "print(\"Columns with 'Yes' outcomes and their probabilities:\")\n",
    "for column_name, probability in yes_labels_with_probabilities:\n",
    "    print(f\"{column_name}: Probability = {probability:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check average probabilities for each label\n",
    "#label_probabilities = prediction.numpy().mean(axis=0)  # Average probabilities across samples\n",
    "label_probabilities = torch.sigmoid(prediction).mean(axis=0)\n",
    "label_names = Y.columns  # Assuming these are your column names\n",
    "\n",
    "for label, prob in zip(label_names, label_probabilities):\n",
    "    print(f\"{label}: Average Probability = {prob:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
